#build_index.py

import os
import json
from tqdm import tqdm
from langchain_huggingface import HuggingFaceEmbeddings

from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_core.documents import Document

# ------------------------------------------
# ğŸ”§ Initialize the embedding model
# ------------------------------------------
embeddings = HuggingFaceEmbeddings(
    model_name="sentence-transformers/all-MiniLM-L6-v2"
)

# ------------------------------------------
# ğŸ“¥ Load JSONL files
# ------------------------------------------
def load_jsonl_file(filepath):
    if not os.path.exists(filepath):
        raise FileNotFoundError(f"âŒ File not found: {filepath}")
    
    with open(filepath, "r", encoding="utf-8") as f:
        lines = [json.loads(line.strip()) for line in f if line.strip()]
    return lines

# ------------------------------------------
# ğŸ“¦ Build the vector index
# ------------------------------------------
def build_index(course_path, forum_path):
    print(f"ğŸ” Checking for files...")
    for path in [course_path, forum_path]:
        if not os.path.exists(path):
            raise FileNotFoundError(f"âŒ Missing file: {path}")
        print(f"âœ… Found: {os.path.basename(path)} at {path}")

    print("ğŸ“¥ Loading data...")
    course_data = load_jsonl_file(course_path)
    forum_data = load_jsonl_file(forum_path)

    print("ğŸ“š Creating documents...")
    documents = []

    skipped = 0

    for item in course_data:
        text = item.get("text") or item.get("content")
        if not text:
            skipped += 1
            continue
        documents.append(Document(
            page_content=text,
            metadata={"source": "course", "title": item.get("title", "")}
        ))

    for item in forum_data:
        text = item.get("text") or item.get("content")
        if not text:
            skipped += 1
            continue
        documents.append(Document(
            page_content=text,
            metadata={"source": "forum", "title": item.get("topic_title", "")}
        ))

    print(f"ğŸ“„ Loaded {len(documents)} valid documents, skipped {skipped} incomplete ones.")

    if not documents:
        raise ValueError("âŒ No valid documents found. Check your JSONL data formatting.")

    print("ğŸ§  Generating embeddings...")
    db = FAISS.from_documents(documents, embeddings)

    os.makedirs("vectorstore", exist_ok=True)
    db.save_local("vectorstore")
    print("âœ… FAISS index built and saved to 'vectorstore/'")

# ------------------------------------------
# ğŸš€ Main execution
# ------------------------------------------
if __name__ == "__main__":
    build_index("data/tds-course-content.jsonl", "data/tds-forum-posts.jsonl")